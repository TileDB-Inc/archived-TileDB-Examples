{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI Ingestion\n",
    "\n",
    "In this tutorial we'll utilize the command-line interface (CLI) and walk through the process of loading raw VCF data into TileDB.\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Hint: By the way, you can launch an interactive version of this tutoral using [TileDB Cloud Notebooks](https://console.tiledb.com/notebooks). Start a session with the *Genomics & Geospatial* image and use the integrated file browser to open the notebook: `examples/genomics/01-vcf-cli.ipynb`\n",
    "\n",
    "Verify the TileDB-VCF CLI is available on your system by printing out its current version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TileDB-VCF version 1b66518-modified\n",
      "TileDB version 2.2.7\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the command wasn't found head to the [Setup section]() of this tutorial.\n",
    "\n",
    "# Create a New Dataset\n",
    "\n",
    "The first step is to **create** an empty dataset. Run the tiledbvcf create command to create a new TileDB-VCF dataset called `small_dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf create --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a new folder in your current working directory called `small_dataset` containing the grouped TileDB arrays that TileDB-VCF will use to store the genomic variant data for any ingested samples (see Data Model for a complete description of how VCF data stored). \n",
    "\n",
    "# Ingest Local VCF Files\n",
    "\n",
    "We'll start with a small example using 3 synthetic VCF files obtained in the previous section. Before proceeding, verify your current working directory contains the `vcfs/` folder with 3 sample VCF files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "gene-promoters-hg38.bed  s3-bcf-samples.txt  vcfs\n",
      "\n",
      "data/vcfs:\n",
      "G1.vcf.gz  G1.vcf.gz.csi  G2.vcf.gz  G2.vcf.gz.csi  G3.vcf.gz  G3.vcf.gz.csi\n"
     ]
    }
   ],
   "source": [
    "ls -R data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Index files are required for ingestion. If your VCF/BCF files have not been indexed you can use [`bcftools`](https://samtools.github.io/bcftools/bcftools.html) to do so:\n",
    "\n",
    "```shell\n",
    "for f in data/vcfs/*.vcf.gz; do bcftools index -c $f; done\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to actually ingest the VCF data into TileDB by running the `store` command and providing a list of VCF files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiledbvcf store --uri small_dataset data/vcfs/G*.vcf.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! Let's verify evertyhing went okay using the `stat` command to provide high-level statistics about our dataset including the number of samples it contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for dataset 'small_dataset':\n",
      "- Version: 4\n",
      "- Tile capacity: 10,000\n",
      "- Anchor gap: 1,000\n",
      "- Number of samples: 3\n",
      "- Extracted attributes: none\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf stat --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have successfully created and populated a TileDB VCF dataset using data stored locally on your machine. Next we'll look at the more common scenario of working with files stored on a cloud object store. \n",
    "\n",
    "# Incremental Updates\n",
    "\n",
    "This next section of the tutorial highlights two key advantages of TileDB as a data store for genomic variant data:\n",
    "\n",
    "- existing datasets can be efficiently updated with new samples\n",
    "- native support for cloud object stores\n",
    "\n",
    "Once a dataset is created, you can easily add new samples to it by re-running the `store` command. Here, we'll add the following additional samples, which are located on [AWS S3](https://aws.amazon.com/s3/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G4.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G5.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G6.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G7.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G8.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G9.bcf\n",
      "s3://tiledb-inc-demo-data/examples/notebooks/vcfs/G10.bcf\n"
     ]
    }
   ],
   "source": [
    "cat data/s3-bcf-samples.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: Samples in this second batch are stored as `BCF` files which are also supported by TileDB-VCF.*\n",
    "\n",
    "This process is identical to the steps perfomed above, the only change needed to our code is passing the file and providing the URLs for the remote files. In this case, we'll simply pass the `s3-bcf-samples.txt` file, which includes a list of the BCF files we want to ingest.\n",
    "\n",
    "You can add the `--verbose` flag to print out more information during the `store` phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization completed in 1.27754 sec.\n",
      "Recorded 60 cells for contig 1 (task 1 / 32)\n",
      "Finalizing contig 1\n",
      "Recorded 51 cells for contig 10 (task 19 / 32)\n",
      "Finalizing contig 10\n",
      "Recorded 68 cells for contig 11 (task 15 / 32)\n",
      "Finalizing contig 11\n",
      "Recorded 70 cells for contig 12 (task 11 / 32)\n",
      "Finalizing contig 12\n",
      "Recorded 59 cells for contig 13 (task 6 / 32)\n",
      "Finalizing contig 13\n",
      "Recorded 52 cells for contig 14 (task 30 / 32)\n",
      "Finalizing contig 14\n",
      "Recorded 65 cells for contig 15 (task 20 / 32)\n",
      "Finalizing contig 15\n",
      "Recorded 58 cells for contig 16 (task 9 / 32)\n",
      "Finalizing contig 16\n",
      "Recorded 65 cells for contig 17 (task 28 / 32)\n",
      "Finalizing contig 17\n",
      "Recorded 58 cells for contig 18 (task 13 / 32)\n",
      "Finalizing contig 18\n",
      "Recorded 55 cells for contig 19 (task 29 / 32)\n",
      "Finalizing contig 19\n",
      "Recorded 71 cells for contig 2 (task 9 / 32)\n",
      "Finalizing contig 2\n",
      "Recorded 69 cells for contig 20 (task 26 / 32)\n",
      "Finalizing contig 20\n",
      "Recorded 61 cells for contig 21 (task 7 / 32)\n",
      "Finalizing contig 21\n",
      "Recorded 53 cells for contig 22 (task 17 / 32)\n",
      "Finalizing contig 22\n",
      "Recorded 56 cells for contig 3 (task 28 / 32)\n",
      "Finalizing contig 3\n",
      "Recorded 55 cells for contig 4 (task 4 / 32)\n",
      "Finalizing contig 4\n",
      "Recorded 65 cells for contig 5 (task 11 / 32)\n",
      "Finalizing contig 5\n",
      "Recorded 68 cells for contig 6 (task 16 / 32)\n",
      "Finalizing contig 6\n",
      "Recorded 56 cells for contig 7 (task 19 / 32)\n",
      "Finalizing contig 7\n",
      "Recorded 53 cells for contig 8 (task 19 / 32)\n",
      "Finalizing contig 8\n",
      "Recorded 66 cells for contig 9 (task 17 / 32)\n",
      "Finalizing contig 9\n",
      "Making sure all finalize tasks completed...\n",
      "All finalize tasks successfully completed. Waited for 0.000212697 sec.\n",
      "Done. Ingested 1,334 records (+ 65,921 anchors) from 7 samples in 31.8802 seconds.\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf store \\\n",
    "  --uri small_dataset \\\n",
    "  --samples-file data/s3-bcf-samples.txt \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's run the `stat` command to verify our dataset now includes 10 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for dataset 'small_dataset':\n",
      "- Version: 4\n",
      "- Tile capacity: 10,000\n",
      "- Anchor gap: 1,000\n",
      "- Number of samples: 10\n",
      "- Extracted attributes: none\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf stat --uri small_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you've updated an existing dataset by adding new *BCF files* located on a *remote* server. Because TileDB is designed to be updatable, this process happens efficiently and without ever touching the previously ingested data—avoiding computationally expensive operations like regenerating combined VCF files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query and Export\n",
    "\n",
    "Now that you've created a TileDB-VCF dataset, you can begin to query the variant data for the 10 ingested samples. As mentioned earlier, several APIs are available for accessing the data. See the [Python usage section](https://docs.tiledb.com/genomics/usage/python) for examples using the `tiledbvcf` Python module. Here, we'll continue working with the CLI, focusing now on the `export` command, which can produce 3 different types of outputs for any given query:\n",
    "\n",
    "1. VCF (or BCF), producing one `.vcf` output file per exported sample\n",
    "2. Tabular, producing a single tab-separated value (TSV) text file containing all intersecting records across the exported samples\n",
    "3. Count-only, outputs a count of the total number of intersecting records (no output file is produced)\n",
    "\n",
    "The following examples provide a high-level overview of the CLI's export functionality; see `tiledbvcf export --help` or the [TileDB-VCF website](https://docs.tiledb.com/genomics/apis/cli) for comprehensive documentation.\n",
    "\n",
    "## Export VCF/BCFs\n",
    "\n",
    "In this example we will export several genomic regions from `small_dataset` created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TileDB query with 3 column ranges.\n",
      "Processed 315 cells in 0.000105732 sec. Reported 13 cells.\n",
      "Done. Exported 13 records in 0.373146 seconds.\n"
     ]
    }
   ],
   "source": [
    "mkdir -p data/exported-subsets\n",
    "\n",
    "tiledbvcf export \\\n",
    "  --uri small_dataset \\\n",
    "  --regions 1:1-50000,2:1-50000,3:1-50000 \\\n",
    "  --sample-names G1,G2,G3 \\\n",
    "  --output-dir data/exported-subsets \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This produced the three `bcf` files shown below, each of which contains the records intersecting the specified regions for the corresponding sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/exported-subsets\n",
      "├── G1.bcf\n",
      "├── G2.bcf\n",
      "└── G3.bcf\n",
      "\n",
      "0 directories, 3 files\n"
     ]
    }
   ],
   "source": [
    "tree data/exported-subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use `bcftools` to examine any of the exported `bcf` files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t13350\t.\tA\t<NON_REF>\t.\t.\tEND=36258\tGT:DP:GQ:MIN_DP:PL\t1/0:50:3:43:44,29,99\n",
      "1\t42091\t.\tA\t<NON_REF>\t.\t.\tEND=101445\tGT:DP:GQ:MIN_DP:PL\t0/0:8:91:60:35,62,92\n",
      "2\t11625\t.\tT\t<NON_REF>\t.\t.\tEND=106375\tGT:DP:GQ:MIN_DP:PL\t0/0:27:72:76:70,30,83\n",
      "3\t14580\t.\tT\t<NON_REF>\t.\t.\tEND=86190\tGT:DP:GQ:MIN_DP:PL\t0/1:50:78:41:67,11,43\n"
     ]
    }
   ],
   "source": [
    "bcftools view --no-header data/exported-subsets/G1.bcf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note: If a sample does not contain any intersecting records an output file is still created but it will include 0 records.***\n",
    "\n",
    "In order to export *all* records, simply omit the `--regions` argument. For example, the following recovers the original BCFs for samples `G4` and `G5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TileDB query with 1 column ranges.\n",
      "Processed 20758 cells in 0.000912681 sec. Reported 412 cells.\n",
      "Done. Exported 412 records in 0.352821 seconds.\n"
     ]
    }
   ],
   "source": [
    "mkdir -p data/exported-full\n",
    "\n",
    "tiledbvcf export \\\n",
    "  --uri small_dataset \\\n",
    "  --sample-names G4,G5 \\\n",
    "  --output-dir data/exported-full \\\n",
    "  --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output two BCFs for the corresponding samples that include the complete set of information from the original files. Note, while these exports are *lossless* in terms of the actual data stored, they may not be *identical* to the original files. For example, fields within the `INFO` and `FORMAT` columns may appear in a slightly different order in the exported files.\n",
    "\n",
    "## Export to TSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll look at exporting data from `small_dataset` in *tabular* format, which makes it convenient to load into other tools for downstream analyses. The command arguments are largely the same as for exporting to VCF/BCF, you just need to change the output type to `t` (short for TSV) and specify which VCF fields should be included as columns in the output table. The standard field names are:\n",
    "\n",
    "- `SAMPLE` (always included)\n",
    "- `ID`\n",
    "- `REF`\n",
    "- `ALT`\n",
    "- `QUAL`\n",
    "- `CHR`\n",
    "- `POS`\n",
    "- `FILTER`\n",
    "\n",
    "Fields within the `FORMAT` and `INFO` columns are accessed using special prefixes, `S:` and `I:`, respectively. This notation is used below to include genotype information from the `FORMAT:GT` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized TileDB query with 1 column ranges.\n",
      "SAMPLE\tCHR\tPOS\tI:END\tREF\tS:GT\n",
      "G3\t1\t11758\t92302\tT\t1,1\n",
      "G2\t1\t12410\t50341\tA\t1,1\n",
      "G1\t1\t13350\t36258\tA\t1,0\n",
      "G1\t1\t42091\t101445\tA\t0,0\n",
      "G4\t1\t10485\t40554\tG\t0,1\n",
      "G4\t1\t46864\t86622\tG\t1,0\n",
      "Processed 143 cells in 0.000123401 sec. Reported 6 cells.\n",
      "Done. Exported 6 records in 0.370958 seconds.\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf export \\\n",
    "    --uri small_dataset \\\n",
    "    -Ot --tsv-fields CHR,POS,I:END,REF,S:GT \\\n",
    "    -s G1,G2,G3,G4 \\\n",
    "    --regions 1:1-50000 \\\n",
    "    --verbose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are output in a long form table where each row represents a single variant that overlapped one of the specified query regions for a particular sample. \n",
    "\n",
    "There's one more special prefix worth pointing out: `Q:` (for *query*), which allows you to include columns containing the start (`Q:POS`) and end (`Q:END`) positions for the query region in which the variant is located. Here we perform the same query across a few more regions, add the query regions columns, and save the output to `data/exported-regions.tsv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE\tQ:POS\tQ:END\tCHR\tPOS\tI:END\tREF\tS:GT\n",
      "G3\t1\t50000\t1\t11758\t92302\tT\t1,1\n",
      "G2\t1\t50000\t1\t12410\t50341\tA\t1,1\n",
      "G1\t1\t50000\t1\t13350\t36258\tA\t1,0\n",
      "G1\t1\t50000\t1\t42091\t101445\tA\t0,0\n",
      "G4\t1\t50000\t1\t10485\t40554\tG\t0,1\n",
      "G4\t1\t50000\t1\t46864\t86622\tG\t1,0\n",
      "G3\t60000\t100000\t2\t11431\t93855\tA\t1,0\n",
      "G1\t60000\t100000\t2\t11625\t106375\tT\t0,0\n",
      "G2\t60000\t100000\t2\t14907\t92383\tT\t1,1\n",
      "G2\t60000\t100000\t2\t92670\t145136\tC\t0,1\n",
      "G3\t60000\t100000\t2\t93967\t149850\tC\t1,0\n",
      "G4\t60000\t100000\t2\t62758\t132118\tA\t1,1\n",
      "G2\t110000\t160000\t3\t78418\t148323\tG\t0,0\n",
      "G3\t110000\t160000\t3\t111926\t162835\tA\t0,0\n",
      "G1\t110000\t160000\t3\t112593\t174752\tT\t1,0\n",
      "G4\t110000\t160000\t3\t91891\t169481\tA\t1,0\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf export \\\n",
    "    --uri small_dataset \\\n",
    "    -Ot --tsv-fields Q:POS,Q:END,CHR,POS,I:END,REF,S:GT \\\n",
    "    -s G1,G2,G3,G4 \\\n",
    "    --regions 1:1-50000,2:60000-100000,3:110000-160000 \\\n",
    "    --output-path data/exported-regions.tsv\n",
    "\n",
    "cat data/exported-regions.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row shows that sample `G3` contains a variant stretching from 11,758–92,302 on chromosome 1 overlaps the first query region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated: Fri Aug 14 13:19:54 UTC 2020\n"
     ]
    }
   ],
   "source": [
    "echo \"Updated: $(date)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TileDB-VCF build d677056\n",
      "TileDB version 2.0.7\n"
     ]
    }
   ],
   "source": [
    "tiledbvcf --version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
